import torch
from torch.utils.data import DataLoader
from transformers import AutoTokenizer
from datasets import Dataset
import os
from tqdm import tqdm
import numpy as np
import matplotlib.pyplot as plt
import gc
import json
from ast_simplifier import JavaCodeSimplifier
from model_search import CodeSearchModel

# ================= 1. å…¨å±€é…ç½® (åœ¨æ­¤ä¿®æ”¹) =================
# æœ¬åœ°æ¨¡åž‹è·¯å¾„ (è¯·ä¿®æ”¹ä¸ºæ‚¨å®žé™…çš„æœ¬åœ°è·¯å¾„)
LOCAL_MODEL_PATHS = {
    "codebert": "./codebert-base-local",  # è¯·ç¡®ä¿å·²ä¸‹è½½å¹¶è§£åŽ‹åˆ°æ­¤æ–‡ä»¶å¤¹
    "codet5": "./codet5-base-local"       # æ‚¨å·²æœ‰çš„ CodeT5 è·¯å¾„
}

# æ•°æ®æ–‡ä»¶è·¯å¾„
TRAIN_FILE = "java_research/train.txt"   # æ‚¨çš„ txt è®­ç»ƒæ–‡ä»¶
TEST_FILE = "java_research/java_test_0.jsonl"   # æ‚¨çš„ jsonl æµ‹è¯•æ–‡ä»¶

RATIOS_TO_TEST = [0,0.1,0.3,0.5]
MODELS_TO_TEST = [ "codet5"]

BATCH_SIZE = 16
EPOCHS = 3
LR = 2e-5
MAX_LENGTH = 256
NUM_WORKERS = 0 

# æ˜¾å¡åŠ é€Ÿé…ç½®
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

# ================= 2. æ•°æ®è¯»å–å‡½æ•° (æ ¸å¿ƒä¿®æ”¹) =================
def load_custom_dataset():
    """
    è‡ªå®šä¹‰åŠ è½½é€»è¾‘ï¼š
    - Train: è¯»å– .txt æ–‡ä»¶
    - Test: è¯»å– .jsonl æ–‡ä»¶
    """
    print(f"Loading datasets...")
    
    # --- 1. åŠ è½½è®­ç»ƒé›† (TXT) ---
    train_codes = []
    train_docs = []
    
    # å‡è®¾ txt æ–‡ä»¶æ¯ä¸€è¡Œæ˜¯ç”¨ TAB åˆ†éš”çš„: "code \t docstring"
    # å¦‚æžœæ‚¨çš„æ ¼å¼ä¸åŒ (æ¯”å¦‚ä¸¤è¡Œä¸€æ¡)ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œæˆ‘å†æ”¹
    with open(TRAIN_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line: continue
            
            # å°è¯•æŒ‰ TAB åˆ†å‰²
            parts = line.split('\t')
            if len(parts) >= 2:
                # å‡è®¾ç¬¬ä¸€åˆ—æ˜¯ codeï¼Œç¬¬äºŒåˆ—æ˜¯ doc (æ ¹æ®å®žé™…æƒ…å†µè°ƒæ•´)
                train_codes.append(parts[0]) 
                train_docs.append(parts[1])
            else:
                # å¦‚æžœæ²¡æœ‰ TABï¼Œå¯èƒ½è¿™è¡Œå…¨æ˜¯ä»£ç ï¼Œæˆ–è€…æ ¼å¼ä¸å¯¹ï¼Œæš‚ä¸”è·³è¿‡æˆ–ä½œä¸ºå•åˆ—å¤„ç†
                pass 
    
    print(f"  Loaded {len(train_codes)} training samples from txt.")
    train_dataset = Dataset.from_dict({"code": train_codes, "docstring": train_docs})

    # --- 2. åŠ è½½æµ‹è¯•é›† (JSONL) ---
    test_codes = []
    test_docs = []
    
    with open(TEST_FILE, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if not line: continue
            try:
                item = json.loads(line)
                # å…¼å®¹ä¸åŒçš„ key åï¼Œæ¯”å¦‚æœ‰çš„æ•°æ®é›†å« 'code_tokens' æœ‰çš„å« 'code'
                c = item.get('code') or item.get('function_tokens') or ""
                d = item.get('docstring') or item.get('docstring_tokens') or ""
                
                # å¦‚æžœæ˜¯ list (tokenized)ï¼Œè½¬å›ž string
                if isinstance(c, list): c = " ".join(c)
                if isinstance(d, list): d = " ".join(d)
                
                test_codes.append(c)
                test_docs.append(d)
            except:
                continue
                
    print(f"  Loaded {len(test_codes)} test samples from jsonl.")
    test_dataset = Dataset.from_dict({"code": test_codes, "docstring": test_docs})
    
    return train_dataset, test_dataset


# ================= 3. è¾…åŠ©è¯„ä¼°ä¸Žè®­ç»ƒé€»è¾‘ =================
def evaluate_mrr(model, dataloader):
    """è®¡ç®— MRR æŒ‡æ ‡"""
    model.eval()
    code_vecs = []
    query_vecs = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="  Evaluating", leave=False):
            code_inputs, query_inputs = batch
            code_inputs = {k: v.to(DEVICE) for k, v in code_inputs.items()}
            query_inputs = {k: v.to(DEVICE) for k, v in query_inputs.items()}
            
            c_vec = model.get_embeddings(code_inputs['input_ids'], code_inputs['attention_mask'])
            q_vec = model.get_embeddings(query_inputs['input_ids'], query_inputs['attention_mask'])
            
            code_vecs.append(c_vec.cpu().numpy())
            query_vecs.append(q_vec.cpu().numpy())
            
    code_vecs = np.concatenate(code_vecs, 0)
    query_vecs = np.concatenate(query_vecs, 0)
    
    scores = np.matmul(query_vecs, code_vecs.T)
    
    ranks = []
    for i in range(len(scores)):
        score_row = scores[i]
        sorted_indices = np.argsort(-score_row)
        rank = np.where(sorted_indices == i)[0][0] + 1
        ranks.append(1.0 / rank)
        
    return np.mean(ranks)


def run_single_experiment(model_type, ratio, train_ds_raw, test_ds_raw):
    """è¿è¡Œå•ä¸ªå®žéªŒ"""
    print(f"\n{'-'*60}")
    print(f"ðŸš€ Experiment: Model={model_type}, Ratio={ratio}")
    print(f"{'-'*60}")
    
    model_path = LOCAL_MODEL_PATHS[model_type]
    # å¦‚æžœæœ¬åœ°æ²¡æœ‰ï¼Œå›žé€€åˆ°åœ¨çº¿åŠ è½½
    if not os.path.exists(model_path):
        print(f"Warning: Local path {model_path} not found. Downloading from HuggingFace...")
        model_path = "microsoft/codebert-base" if model_type == "codebert" else "Salesforce/codet5-base"

    tokenizer = AutoTokenizer.from_pretrained(model_path)
    simplifier = JavaCodeSimplifier()
    
    # --- æ•°æ®å¤„ç† ---
    def preprocess_batch(batch):
        codes = batch['code']
        docstrings = batch['docstring']
        simplified_codes = []
        for c in codes:
            try:
                s_code = simplifier.simplify(c, remove_ratio=ratio)
                simplified_codes.append(str(s_code))
            except:
                simplified_codes.append("")
        
        cleaned_docs = []
        for d in docstrings:
            d = str(d) if d is not None else ""
            cleaned_docs.append(d.split('\n')[0]) 
            
        return simplified_codes, cleaned_docs

    def collate_fn(batch):
        code_list = [item['code_simplified'] for item in batch]
        doc_list = [item['doc_cleaned'] for item in batch]
        
        code_inputs = tokenizer(code_list, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt")
        query_inputs = tokenizer(doc_list, padding=True, truncation=True, max_length=MAX_LENGTH, return_tensors="pt")
        return code_inputs, query_inputs

    def map_function(examples):
        c, d = preprocess_batch(examples)
        return {"code_simplified": c, "doc_cleaned": d}

    # å¤„ç†æ•°æ®é›†
    train_ds = train_ds_raw.map(map_function, batched=True, batch_size=100, load_from_cache_file=False, desc="Processing Train")
    test_ds = test_ds_raw.map(map_function, batched=True, batch_size=100, load_from_cache_file=False, desc="Processing Test")
    
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS)
    test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=NUM_WORKERS)
    
    # --- æ¨¡åž‹è®­ç»ƒ ---
    model = CodeSearchModel(model_path, model_type=model_type)
    if hasattr(model.encoder, "gradient_checkpointing_enable"):
        model.encoder.gradient_checkpointing_enable()
        
    model.to(DEVICE)
    optimizer = torch.optim.AdamW(model.parameters(), lr=LR)
    
    best_mrr = 0.0
    
    for epoch in range(EPOCHS):
        model.train()
        total_loss = 0
        pbar = tqdm(train_loader, desc=f"  Epoch {epoch+1}/{EPOCHS}", leave=False)
        for batch in pbar:
            code_inputs, query_inputs = batch
            code_inputs = {k: v.to(DEVICE) for k, v in code_inputs.items()}
            query_inputs = {k: v.to(DEVICE) for k, v in query_inputs.items()}
            
            optimizer.zero_grad()
            loss = model(code_inputs, query_inputs)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
            pbar.set_postfix({"loss": f"{loss.item():.4f}"})
        
        mrr = evaluate_mrr(model, test_loader)
        print(f"  Epoch {epoch+1} finished. MRR: {mrr:.4f}")
        if mrr > best_mrr: best_mrr = mrr
            
    # æ¸…ç†
    del model, optimizer, train_loader, test_loader
    gc.collect()
    torch.cuda.empty_cache()
    
    return best_mrr

# ================= 4. ä¸»ç¨‹åº =================
if __name__ == "__main__":
    # 1. å…ˆåŠ è½½åŽŸå§‹æ•°æ® (åªåŠ è½½ä¸€æ¬¡ï¼Œä¸ç”¨æ¯æ¬¡å¾ªçŽ¯éƒ½è¯»æ–‡ä»¶)
    raw_train_ds, raw_test_ds = load_custom_dataset()
    

    raw_train_ds = raw_train_ds.select(range(1000))
    raw_test_ds = raw_test_ds.select(range(100))

    results = {m: [] for m in MODELS_TO_TEST}
    
    try:
        for model_type in MODELS_TO_TEST:
            for ratio in RATIOS_TO_TEST:
                score = run_single_experiment(model_type, ratio, raw_train_ds, raw_test_ds)
                results[model_type].append(score)
    except KeyboardInterrupt:
        print("\nInterrupted.")

    # ç»˜å›¾é€»è¾‘
    print("\nResults:", results)
    if any(len(v) > 0 for v in results.values()):
        plt.figure(figsize=(10, 6))
        for m, scores in results.items():
            if scores:
                plt.plot(RATIOS_TO_TEST[:len(scores)], scores, marker='o', label=m)
        plt.title('Code Search MRR vs Simplification Ratio')
        plt.xlabel('Simplification Ratio')
        plt.ylabel('MRR')
        plt.legend()
        plt.grid(True)
        plt.savefig('search_benchmark_result.png')
        print("Chart saved.")